# In your first notebook cell, include:
%matplotlib inline

import os
import glob
import pandas as pd
import json
import logging
import missingno as msno
import pingouin as pg
import seaborn as sns
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import adfuller
from scipy import stats
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Remove any interactive backend settings or plt.ion() calls so plots render inline.

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    filename='data_processing.log',
    filemode='w'
)

def load_and_integrate_json_data(data_folder):
    """Loads and integrates electricity data from JSON files."""
    all_dataframes = []
    
    try:
        for filename in glob.glob(os.path.join(data_folder, '*.json')):
            try:
                with open(filename, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if not isinstance(data, dict) or 'response' not in data or 'data' not in data['response']:
                        continue
                    df = pd.json_normalize(data['response']['data'])
                    # Convert value column to numeric
                    df['value'] = pd.to_numeric(df['value'], errors='coerce')
                    all_dataframes.append(df)
            except Exception as e:
                logging.error(f"Error loading {filename}: {e}")
                continue

        if not all_dataframes:
            return None

        integrated_df = pd.concat(all_dataframes, ignore_index=True)
        integrated_df['period'] = pd.to_datetime(integrated_df['period'], format='%Y-%m-%dT%H', errors='coerce')
        return integrated_df.rename(columns={
            'period': 'timestamp',
            'value': 'electricity_usage'
        }).dropna(subset=['timestamp'])

    except Exception as e:
        logging.error(f"JSON processing error: {e}")
        return None

def load_and_integrate_csv_data(data_folder):
    """Loads and integrates weather data from CSV files."""
    all_dataframes = []
    
    try:
        for filename in glob.glob(os.path.join(data_folder, '*.csv')):
            try:
                df = pd.read_csv(filename, encoding='utf-8', engine='python', on_bad_lines='skip')
                if {'date', 'temperature_2m'}.issubset(df.columns):
                    df['date'] = pd.to_datetime(df['date'].str.slice(0, 10), errors='coerce')
                    df['temperature_2m'] = pd.to_numeric(df['temperature_2m'], errors='coerce')
                    all_dataframes.append(df.dropna(subset=['date']))
            except Exception as e:
                logging.error(f"Error loading {filename}: {e}")
                continue

        if not all_dataframes:
            return None

        return pd.concat(all_dataframes).rename(columns={
            'date': 'timestamp',
            'temperature_2m': 'temperature'
        })

    except Exception as e:
        logging.error(f"CSV processing error: {e}")
        return None

def detect_outliers_iqr(df, col):
    """Detects outliers using the IQR method."""
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return df[(df[col] < lower_bound) | (df[col] > upper_bound)]

def detect_outliers_zscore(df, col):
    """Detects outliers using the Z-score method."""
    z = np.abs(stats.zscore(df[col].dropna()))
    return df[z > 3]

def preprocess_data(combined_data):
    """Performs data preprocessing with outlier detection and handling."""
    if combined_data is None:
        return None

    try:
        # Convert to numeric if needed
        numeric_cols = ['electricity_usage', 'temperature']
        for col in numeric_cols:
            if col in combined_data.columns:
                combined_data[col] = pd.to_numeric(combined_data[col], errors='coerce')

        # Feature engineering
        combined_data['timestamp'] = pd.to_datetime(combined_data['timestamp'])
        combined_data['hour'] = combined_data['timestamp'].dt.hour
        combined_data['month'] = combined_data['timestamp'].dt.month
        combined_data['is_weekend'] = combined_data['timestamp'].dt.dayofweek.isin([5, 6]).astype(int)

        # Outlier Detection and Handling
        numerical_cols = combined_data.select_dtypes(include=np.number).columns
        outlier_indices = set()
        
        for col in numerical_cols:
            if col in ['hour', 'month', 'is_weekend']:
                continue

            # Visualize before outlier removal
            fig, ax = plt.subplots(figsize=(10, 5))
            sns.boxplot(x=combined_data[col], ax=ax)
            ax.set_title(f"Boxplot of {col} (Before Outlier Removal)")
            plt.tight_layout()
            plt.show()
            plt.close(fig)

            try:
                Q1 = combined_data[col].quantile(0.25)
                Q3 = combined_data[col].quantile(0.75)
                IQR = Q3 - Q1
                iqr_outliers = combined_data[
                    (combined_data[col] < (Q1 - 1.5 * IQR)) | 
                    (combined_data[col] > (Q3 + 1.5 * IQR))
                ]
                
                z_scores = np.abs(stats.zscore(combined_data[col].dropna()))
                zscore_outliers = combined_data.iloc[np.where(z_scores > 3)[0]]
                
                outlier_indices.update(iqr_outliers.index)
                outlier_indices.update(zscore_outliers.index)
                
            except Exception as e:
                logging.error(f"Outlier detection error for {col}: {e}")
                continue

        combined_data['day_of_week'] = combined_data['timestamp'].dt.dayofweek
        return combined_data.dropna()

    except Exception as e:
        logging.error(f"Preprocessing error: {e}")
        return None

def perform_eda(data):
    """Comprehensive Exploratory Data Analysis with statistical tests and annotations."""
    if data is None or data.empty:
        print("No data available for EDA")
        return

    try:
        data = data.sort_values('timestamp').set_index('timestamp')
        numeric_cols = data.select_dtypes(include=np.number).columns
        
        # 1. Statistical Summary
        print("\n" + "=" * 40)
        print("Statistical Summary")
        print("=" * 40)
        stats_df = data[numeric_cols].describe(percentiles=[.25, .5, .75]).T
        stats_df['skewness'] = data[numeric_cols].skew()
        stats_df['kurtosis'] = data[numeric_cols].kurtosis()
        print(stats_df[['mean', 'std', 'min', '25%', '50%', '75%', 'max', 'skewness', 'kurtosis']])
        
        # 2. Time Series Analysis
        fig, ax = plt.subplots(figsize=(14, 7))
        ax.plot(data.index, data['electricity_usage'], color='#1f77b4', label='Electricity Usage')
        dates = mdates.date2num(data.index)
        z = np.polyfit(dates, data['electricity_usage'], 1)
        p = np.poly1d(z)
        ax.plot(data.index, p(dates), "--", color='#ff7f0e', label='Trend')
        peaks = data.nlargest(3, 'electricity_usage')
        for idx, row in peaks.iterrows():
            ax.annotate(f'Peak: {row["electricity_usage"]:.1f}', 
                        xy=(idx, row["electricity_usage"]),
                        textcoords="offset points", xytext=(0, 10), ha='center',
                        arrowprops=dict(arrowstyle="->", color='red'))
        ax.set_xlabel("Date")
        ax.set_ylabel("Electricity Usage")
        ax.legend(loc='upper left')
        plt.xticks(rotation=45)
        fig.autofmt_xdate()
        plt.tight_layout()
        plt.show()
        plt.close(fig)

        # 3. Univariate Analysis
        fig, axes = plt.subplots(2, 2, figsize=(16, 10))
        sns.histplot(data['electricity_usage'], kde=True, ax=axes[0, 0], color='#2ca02c')
        axes[0, 0].set_title("Electricity Usage Distribution")
        axes[0, 0].axvline(data['electricity_usage'].mean(), color='r', linestyle='--', label='Mean')
        axes[0, 0].axvline(data['electricity_usage'].median(), color='g', linestyle='-', label='Median')
        axes[0, 0].legend(loc='upper right')
        
        sns.boxplot(x=data['temperature'], ax=axes[0, 1], color='#9467bd')
        axes[0, 1].set_title("Temperature Distribution")
        
        sns.kdeplot(data['electricity_usage'], ax=axes[1, 0], color='#e377c2', fill=True)
        axes[1, 0].set_title("Electricity Usage Density Plot")
        
        hourly_avg = data.groupby('hour')['electricity_usage'].mean()
        hourly_avg.plot(ax=axes[1, 1], marker='o', color='#17becf')
        axes[1, 1].set_title("Average Hourly Usage Pattern")
        axes[1, 1].set_xticks(range(0, 24))
        axes[1, 1].grid(True)
        plt.tight_layout()
        plt.show()
        plt.close(fig)

        print("\nDistribution Analysis:")
        print(f"Electricity Usage Skewness: {stats_df.loc['electricity_usage', 'skewness']:.2f}")
        print(f"Electricity Usage Kurtosis: {stats_df.loc['electricity_usage', 'kurtosis']:.2f}")
        print("Temperature IQR: {:.1f} to {:.1f}".format(stats_df.loc['temperature', '25%'], stats_df.loc['temperature', '75%']))

        # 4. Correlation Analysis
        fig, ax = plt.subplots(figsize=(12, 8))
        corr_matrix = data[numeric_cols].corr()
        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', mask=mask, vmin=-1, vmax=1, fmt=".2f", linewidths=.5, ax=ax)
        ax.set_title("Feature Correlation Matrix", pad=20)
        plt.tight_layout()
        plt.show()
        plt.close(fig)

        high_corr = corr_matrix[(abs(corr_matrix) > 0.75) & (corr_matrix < 1.0)]
        if not high_corr.stack().empty:
            print("\nMulticollinearity Alert:")
            print(high_corr.stack().to_string())
        else:
            print("\nNo significant multicollinearity detected (|r| > 0.75)")

        # 5. Advanced Time Series Analysis
        if len(data) > 24 * 7:
            try:
                decomposition = seasonal_decompose(data['electricity_usage'].ffill(), model='additive', period=24)
                fig = decomposition.plot()
                fig.set_size_inches(14, 10)
                fig.suptitle('Time Series Decomposition (Hourly Data)', y=1.02)
                plt.tight_layout()
                plt.show()
                plt.close(fig)
            except Exception as e:
                print(f"Decomposition error: {e}")

            print("\n" + "=" * 40)
            print("Stationarity Analysis (ADF Test)")
            print("=" * 40)
            try:
                result = adfuller(data['electricity_usage'].dropna())
                print(f'ADF Statistic: {result[0]:.4f}')
                print(f'p-value: {result[1]:.4f}')
                print('Critical Values:')
                for key, value in result[4].items():
                    print(f'   {key}: {value:.4f}')
                if result[1] <= 0.05:
                    print("\nConclusion: Series is stationary (p ≤ 0.05)")
                else:
                    print("\nConclusion: Series is non-stationary (p > 0.05)")
            except Exception as e:
                print(f"ADF test error: {e}")
        else:
            print("\nInsufficient data for time series decomposition (need ≥1 week of hourly data)")

    except Exception as e:
        print(f"EDA Error: {str(e)}")
        logging.error(f"EDA failed: {str(e)}")

def build_regression_model(data):
    """Builds and evaluates a regression model to predict electricity demand."""
    if data is None or data.empty:
        print("No data available for regression modeling.")
        return

    try:
        features = ['hour', 'month', 'temperature', 'day_of_week']
        missing_features = [f for f in features if f not in data.columns]
        if missing_features:
            print(f"Missing required features: {missing_features}")
            return

        X = data[features]
        y = data['electricity_usage']
        X = pd.get_dummies(X, columns=['day_of_week'], drop_first=True)
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, 
            test_size=0.2, 
            random_state=42,
            shuffle=False
        )

        model = LinearRegression()
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        
        mse = mean_squared_error(y_test, y_pred)
        rmse = np.sqrt(mse)
        r2 = r2_score(y_test, y_pred)
        mae = np.mean(np.abs(y_test - y_pred))

        print("\n" + "="*40)
        print("Regression Model Evaluation")
        print("="*40)
        print(f"Mean Absolute Error (MAE): {mae:.2f}")
        print(f"Mean Squared Error (MSE): {mse:.2f}")
        print(f"Root Mean Squared Error (RMSE): {rmse:.2f}")
        print(f"R² Score: {r2:.4f}")

        fig, ax = plt.subplots(figsize=(10, 6))
        ax.scatter(y_test, y_pred, alpha=0.5, color='#1f77b4')
        ax.plot([y.min(), y.max()], [y.min(), y.max()], '--r', linewidth=2)
        ax.set_xlabel("Actual Electricity Usage", fontsize=12)
        ax.set_ylabel("Predicted Electricity Usage", fontsize=12)
        ax.set_title("Actual vs. Predicted Values", fontsize=14)
        ax.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()
        plt.close(fig)

        residuals = y_test - y_pred
        fig, ax = plt.subplots(figsize=(10, 6))
        sns.histplot(residuals, kde=True, color='#2ca02c', ax=ax)
        ax.set_xlabel("Residuals", fontsize=12)
        ax.set_ylabel("Density", fontsize=12)
        ax.set_title("Residual Distribution", fontsize=14)
        ax.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()
        plt.close(fig)

        fig, ax = plt.subplots(figsize=(10, 6))
        ax.scatter(y_pred, residuals, alpha=0.5, color='#d62728')
        ax.axhline(y=0, color='black', linestyle='--')
        ax.set_xlabel("Predicted Values", fontsize=12)
        ax.set_ylabel("Residuals", fontsize=12)
        ax.set_title("Residuals vs. Predicted Values", fontsize=14)
        ax.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()
        plt.close(fig)

        print("\n" + "="*40)
        print("Model Coefficients")
        print("="*40)
        coefficients = pd.DataFrame({
            'Feature': X.columns,
            'Coefficient': model.coef_
        }).sort_values('Coefficient', ascending=False)
        print(coefficients)

        print("\n" + "="*40)
        print("Residual Analysis Findings")
        print("="*40)
        print("1. Residual Distribution:")
        if abs(residuals.skew()) > 1:
            print("   - Residuals are significantly skewed (non-normal distribution)")
        else:
            print("   - Residuals show approximately normal distribution")
            
        print("\n2. Residual Patterns:")
        if np.abs(residuals).mean() > 0.1 * y.mean():
            print("   - Large residual magnitude detected (>10% of mean demand)")
        else:
            print("   - Residual magnitudes within acceptable range")
            
        print("\n3. Heteroscedasticity Check:")
        if (residuals.std() / y.std()) > 0.5:
            print("   - Significant variance in residuals detected")
        else:
            print("   - Residual variance appears consistent")

    except Exception as e:
        print(f"Regression modeling error: {e}")
        logging.error(f"Regression modeling failed: {e}")

# Main block
if __name__ == "__main__":
    electricity_data = load_and_integrate_json_data(r'C:\Users\Cp9-30\Desktop\raw\electricity_raw_data')
    weather_data = load_and_integrate_csv_data(r'C:\Users\Cp9-30\Desktop\raw\weather_raw_data')

    if electricity_data is not None and weather_data is not None:
        try:
            merged_data = pd.merge(electricity_data, weather_data, on='timestamp', how='inner')
            cleaned_data = preprocess_data(merged_data)

            if cleaned_data is not None:
                perform_eda(cleaned_data)
                build_regression_model(cleaned_data)
                cleaned_data.to_csv('cleaned_combined_data.csv', index=False)
                print("Processing completed successfully!")
            else:
                print("Data cleaning failed. Check logs for details.")

        except Exception as e:
            print(f"Error: {e}")
    else:
        print("Failed to load input data. Check logs for details.")
